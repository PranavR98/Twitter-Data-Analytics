{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a61ae8-7021-4268-869d-f9f0583568ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:46:39) \n",
      "[GCC 10.4.0]\n",
      "3.1.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4400bfd8-c645-4cc2-801a-48799c292767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import io\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040efc45-9ee4-4e17-b734-f41a0b6bbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.reset_option('display.max_rows')\n",
    "from itertools import compress \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c231c38-c9f5-4327-81ba-863311d4b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b051e43a-2c02-4ea0-a520-759a6f507778",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93941457-8def-4213-bcec-d381ccca8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca33885-3377-4ecd-ac0c-fdf8b69bdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name, folder_name):\n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "\n",
    "    for blob in blobs:\n",
    "        print(blob.name + '\\t' + str(blob.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331005af-426a-4b06-b7e5-95f4e1fcbcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_pd(bucket_name, folder_name):\n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "\n",
    "    blob_name = []\n",
    "    blob_size = []\n",
    "    \n",
    "    for blob in blobs:\n",
    "        blob_name.append(blob.name)\n",
    "        blob_size.append(blob.size)\n",
    "\n",
    "    blobs_df = pd.DataFrame(list(zip(blob_name, blob_size)), columns=['Name','Size'])\n",
    "\n",
    "    blobs_df.style.format({\"Size\": \"{:,.0f}\"}) \n",
    "    \n",
    "    return blobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b0c0a0-fece-4b33-a204-f58b7f0a7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder(bucket_name, folder_name):\n",
    "    gcs_client = storage.Client()\n",
    "    bucket = gcs_client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "\n",
    "    for blob in blobs:\n",
    "        blob.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763d12cc-be51-4f4a-97c1-7170f80425c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50696 items\n",
      "-rwx------   3 root root          0 2023-02-08 13:58 gs://msca-bdp-tweets/final_project/_SUCCESS\n",
      "-rwx------   3 root root    4500466 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00000-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n",
      "-rwx------   3 root root    4107431 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00001-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n",
      "-rwx------   3 root root    4672123 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00002-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n",
      "-rwx------   3 root root    5186684 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00003-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n",
      "-rwx------   3 root root    4729662 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00004-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n",
      "-rwx------   3 root root    4605529 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00005-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n",
      "-rwx------   3 root root    6370756 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00006-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n",
      "-rwx------   3 root root    4894125 2023-02-08 13:44 gs://msca-bdp-tweets/final_project/part-00007-aa6d3cb4-7022-4df2-9921-218307589ce2-c000.json\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls 'gs://msca-bdp-tweets/final_project/' | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91539901-fdbe-416c-ba11-3eee35f888b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from open bucket, avaible to all students\n",
    "bucket_read = 'msca-bdp-tweets'\n",
    "\n",
    "# Saving results into individual bucket, students must update to their own bucket\n",
    "bucket_write = 'msca-bdp-data-shared'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a262ba-c7ef-4c45-8cfd-0c5e53212fe9",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8811ee4-f4b8-45b1-80a6-2d47bd930993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/03 19:37:31 WARN org.apache.spark.sql.execution.datasources.SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "23/03/03 20:07:15 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "tweets_original = spark.read.json('gs://msca-bdp-tweets/final_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78aa5f91-e510-441d-9861-4755804864ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "##Add \"eagerEval.enabled\" to beautify the way Spark DF is displayed\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "\n",
    "## To use legacy casting notation for date\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e0b0ff-98a1-4020-b5db-860da599057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>(5735 + 6) / 5741]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99994342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Checking for the total number of records\n",
    "print(tweets_original.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a58948-95b8-4bc3-a53c-5ddd75dca79f",
   "metadata": {},
   "source": [
    "### Considering only English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6cd75d3-7c85-4d21-a3f4-be0e0fdd55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_original = tweets_original.filter(tweets_original.lang == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2c0124-5c3d-4657-9dee-3304220520de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filt = tweets_original\\\n",
    ".withColumn('tweet_text', lower('tweet_text'))\\\n",
    ".withColumn('stripped', regexp_replace(col(\"tweet_text\"),\"[\\$#,&%\\\".]\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce863f-5b04-4d3a-9db4-d5d925d9d38e",
   "metadata": {},
   "source": [
    "### Filtering out education related tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be545abb-61c4-4c74-aeef-23f4c35cbf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['masters degree',\n",
       " 'government school',\n",
       " 'undergraduate',\n",
       " 'school',\n",
       " 'grades',\n",
       " 'children',\n",
       " 'education',\n",
       " 'degree',\n",
       " 'higher education',\n",
       " 'tutor',\n",
       " 'educate',\n",
       " 'education news',\n",
       " 'lecturer',\n",
       " 'college',\n",
       " 'educator',\n",
       " 'curriculum',\n",
       " 'student',\n",
       " 'lecture',\n",
       " 'training',\n",
       " 'teach',\n",
       " 'teacher',\n",
       " 'tuition',\n",
       " 'courses',\n",
       " 'phd',\n",
       " 'educational',\n",
       " 'university',\n",
       " 'secondary school',\n",
       " 'research',\n",
       " 'exam',\n",
       " 'classroom',\n",
       " 'graduate school',\n",
       " 'primary school',\n",
       " 'bachelors degreelibrary',\n",
       " 'studying',\n",
       " 'professor',\n",
       " 'learn',\n",
       " 'public school']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edu_words = ['education', 'educator', 'exam', 'classroom', 'learn', 'degree', 'education news', 'student', 'university', 'college', 'teacher', \n",
    "          'professor', 'phd', 'masters degree', 'bachelors degree' 'library', 'research', 'graduate school', 'undergraduate', 'teach', \n",
    "         'school', 'training', 'public school', 'educational', 'primary school', 'secondary school', 'higher education', 'tuition', 'courses',\n",
    "        'exam', 'lecturer', 'lecture', 'educate', 'children', 'studying', 'government school', 'curriculum', 'tuition', 'tutor','grades']\n",
    "\n",
    "edu_words = [word.lower().replace('_', ' ') for word in list(set(edu_words))]\n",
    "edu_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ceebfa0-85f8-4a43-9afb-d6eeb795587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "@f.udf(returnType=IntegerType())\n",
    "def count_words(tweet):\n",
    "    tweet = list(tweet.split())\n",
    "    count = 0\n",
    "    for word in edu_words: # Use list created in the global scope\n",
    "        count += tweet.count(word)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f394bb4-5b07-4999-bfdf-446d978cdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filt = tweets_filt.withColumn('edu_count', count_words('tweet_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c28ce9-de3f-4eb8-a1c3-e47530c3181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for tweets that contain at least 2 education related words\n",
    "tweets_filt = tweets_filt.filter(f.col('edu_count') >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e0e0908-ae8b-4359-903d-e00dbd907608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16114741"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a95c42-6a0e-4eaa-a437-ed476572909d",
   "metadata": {},
   "source": [
    "### Removing non-educational tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86369ef-4269-47cc-a61d-67731a9fd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_words = ['guns', 'fashion', 'gaming', 'makeup', 'shooting', 'sports', 'business', 'gun', 'kill', 'food',\n",
    "                'news', 'travel', 'killed', 'murder', 'uvalde', 'health', 'shoot', 'deceased', 'movie', 'politics', 'beauty', 'horny',\n",
    "                'shootings', 'gunned', 'fitness', 'music', 'shopping', 'attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e3887b-1295-4195-b586-ef34f2501402",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_removal ='|'.join([\"(\" + c +\")\" for c in removal_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c70f7820-a672-4352-8b59-42558d0610c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filt = tweets_filt.where(~tweets_filt['tweet_text'].rlike(regex_removal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6c314-8b36-4268-892a-24ada30ad04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/03 20:32:33 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 34 for reason Container marked as failed: container_1677865265526_0008_01_000034 on host: hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "23/03/03 20:32:33 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 34 on hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal: Container marked as failed: container_1677865265526_0008_01_000034 on host: hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "23/03/03 20:32:33 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 32 for reason Container marked as failed: container_1677865265526_0008_01_000032 on host: hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "23/03/03 20:32:33 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 32 on hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal: Container marked as failed: container_1677865265526_0008_01_000032 on host: hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "23/03/03 20:34:04 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 7.0 (TID 27228) (hub-msca-bdp-dphub-students-pranavr569-sw-lpvb.c.msca-bdp-students.internal executor 27): FetchFailed(BlockManagerId(34, hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal, 7337, None), shuffleId=1, mapIndex=8, mapId=21491, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failed to connect to hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal/10.128.0.108:7337\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: Failed to connect to hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal/10.128.0.108:7337\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:287)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockStoreClient.lambda$fetchBlocks$0(ExternalBlockStoreClient.java:105)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:153)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:181)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal/10.128.0.108:7337\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      ")\n",
      "23/03/03 20:34:58 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 7.1 (TID 27580) (hub-msca-bdp-dphub-students-pranavr569-sw-rp91.c.msca-bdp-students.internal executor 25): FetchFailed(BlockManagerId(32, hub-msca-bdp-dphub-students-pranavr569-sw-n2qq.c.msca-bdp-students.internal, 7337, None), shuffleId=1, mapIndex=4, mapId=21487, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=2125898710000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1677865265526_0008, execId=32)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n",
      "\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n",
      "\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=2125898710000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1677865265526_0008, execId=32)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n",
      "\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n",
      "\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n",
      "\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n",
      "\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13128895"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6227a8-e451-40db-8731-5c843c462362",
   "metadata": {},
   "source": [
    "### Writing the filtered data to disk as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e42e3-0348-42c4-a6c6-ea77e91f4b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_filt.write.format(\"parquet\").\\\n",
    "mode('overwrite').\\\n",
    "save('gs://' + 'msca-bdp-students-bucket/shared_data/pranavr569' + '/tweets_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3eed623-ff2d-4443-92ef-6ebe1cd0cdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5742 items\n",
      "-rwx------   3 root root          0 2023-03-03 21:01 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/_SUCCESS\n",
      "-rwx------   3 root root    2364455 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00000-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n",
      "-rwx------   3 root root    2651042 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00001-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n",
      "-rwx------   3 root root    3018283 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00002-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n",
      "-rwx------   3 root root    2582534 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00003-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n",
      "-rwx------   3 root root    2621646 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00004-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n",
      "-rwx------   3 root root    2649380 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00005-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n",
      "-rwx------   3 root root    2771384 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00006-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n",
      "-rwx------   3 root root    2606708 2023-03-03 20:37 gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered/part-00007-554fe560-9be0-449a-a51f-ee5129f09842-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls 'gs://msca-bdp-students-bucket/shared_data/pranavr569/tweets_filtered' | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44d048-5d22-4af1-8d9d-7771076182d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
